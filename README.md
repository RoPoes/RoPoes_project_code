# ROPOES: ROBOT JOINT POSE ESTIMATION

In this paper, we present a method to estimate the position state vector of the robotic manipulator. We propose a multi-camera setup to estimate joint angles and track joint poses in the scene. We use DRAKE for simulating KUKA IIWA and a two-camera system to track the manipulator as it moves in its configuration space. An hourglass network estimates 2D joint locations in a camera frame, and the intersection of their back-projected rays gives the 3D joint location estimates. We generate the dataset for training the hourglass network - 7K images of the manipulator in various configurations - by reorienting the camera to different locations over a hemispherical dome centered at the manipulator base. Ground truth(3D points) is generated from the simulator and trains the belief map estimation for each joint in the arm. We calculate the joint angles using the 3D key points and kinematic model. This work serves as the foundation for the egocentric supervision of robotic manipulators.

